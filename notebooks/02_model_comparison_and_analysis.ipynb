{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison and Analysis\n",
    "\n",
    "This notebook provides comprehensive comparison and analysis of different model architectures\n",
    "for medical image analysis, including U-Net and DeepLabV3+ models.\n",
    "\n",
    "## Requirements Addressed:\n",
    "- 5.1: Visualize model predictions and performance metrics\n",
    "- 5.2: Generate loss curves and metric plots\n",
    "- 5.3: Create confusion matrices and ROC curves\n",
    "- 5.4: Generate overlay visualizations with color-coded regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from src.model import UNet, DeepLabV3Plus, create_model, model_summary, compare_models\n",
    "from src.dataset import MedicalImageDataset\n",
    "from src.metrics import MedicalMetrics\n",
    "from src.visualization import VisualizationEngine\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different model configurations\n",
    "model_configs = {\n",
    "    'UNet_Small': {\n",
    "        'model': {\n",
    "            'architecture': 'unet',\n",
    "            'in_channels': 1,\n",
    "            'num_classes': 2,\n",
    "            'depth': 3,\n",
    "            'start_filters': 32,\n",
    "            'task_type': 'classification'\n",
    "        }\n",
    "    },\n",
    "    'UNet_Medium': {\n",
    "        'model': {\n",
    "            'architecture': 'unet',\n",
    "            'in_channels': 1,\n",
    "            'num_classes': 2,\n",
    "            'depth': 4,\n",
    "            'start_filters': 64,\n",
    "            'task_type': 'classification'\n",
    "        }\n",
    "    },\n",
    "    'UNet_Large': {\n",
    "        'model': {\n",
    "            'architecture': 'unet',\n",
    "            'in_channels': 1,\n",
    "            'num_classes': 2,\n",
    "            'depth': 5,\n",
    "            'start_filters': 64,\n",
    "            'task_type': 'classification'\n",
    "        }\n",
    "    },\n",
    "    'DeepLabV3Plus': {\n",
    "        'model': {\n",
    "            'architecture': 'deeplabv3',\n",
    "            'in_channels': 1,\n",
    "            'num_classes': 2,\n",
    "            'task_type': 'classification'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create models and get summaries\n",
    "model_summaries = {}\n",
    "models = {}\n",
    "\n",
    "for name, config in model_configs.items():\n",
    "    print(f\"Creating {name}...\")\n",
    "    try:\n",
    "        model = create_model(config)\n",
    "        models[name] = model\n",
    "        summary = model_summary(model, input_size=(1, 1, 256, 256))\n",
    "        model_summaries[name] = summary\n",
    "        print(f\"  ✓ {name}: {summary['total_parameters']:,} parameters, {summary['model_size_mb']:.1f} MB\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed to create {name}: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully created {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for name, summary in model_summaries.items():\n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'Architecture': summary['model_name'],\n",
    "        'Parameters': f\"{summary['total_parameters']:,}\",\n",
    "        'Size (MB)': f\"{summary['model_size_mb']:.1f}\",\n",
    "        'Output Shape': str(summary['output_shapes'])\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Architecture Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model complexity comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "model_names = list(model_summaries.keys())\n",
    "parameters = [summary['total_parameters'] for summary in model_summaries.values()]\n",
    "sizes_mb = [summary['model_size_mb'] for summary in model_summaries.values()]\n",
    "\n",
    "# Parameter count comparison\n",
    "bars1 = axes[0].bar(model_names, parameters, color='skyblue', alpha=0.8)\n",
    "axes[0].set_title('Model Parameter Count', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Parameters')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, param in zip(bars1, parameters):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{param/1e6:.1f}M', ha='center', va='bottom')\n",
    "\n",
    "# Model size comparison\n",
    "bars2 = axes[1].bar(model_names, sizes_mb, color='lightcoral', alpha=0.8)\n",
    "axes[1].set_title('Model Size (MB)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Size (MB)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, size in zip(bars2, sizes_mb):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{size:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# Parameter efficiency (parameters per MB)\n",
    "efficiency = [p/s for p, s in zip(parameters, sizes_mb)]\n",
    "bars3 = axes[2].bar(model_names, efficiency, color='lightgreen', alpha=0.8)\n",
    "axes[2].set_title('Parameter Efficiency\\n(Parameters per MB)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylabel('Parameters per MB')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, eff in zip(bars3, efficiency):\n",
    "    height = bar.get_height()\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{eff/1e6:.1f}M', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference Speed Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference speed for different models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Testing inference speed on: {device}\")\n",
    "\n",
    "# Create dummy input\n",
    "batch_sizes = [1, 4, 8, 16]\n",
    "input_size = (256, 256)\n",
    "\n",
    "speed_results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"\\nTesting {model_name}...\")\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        dummy_input = torch.randn(batch_size, 1, *input_size).to(device)\n",
    "        \n",
    "        # Warm up\n",
    "        with torch.no_grad():\n",
    "            for _ in range(5):\n",
    "                _ = model(dummy_input)\n",
    "        \n",
    "        # Time inference\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(20):\n",
    "                _ = model(dummy_input)\n",
    "        \n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_time = (end_time - start_time) / 20\n",
    "        fps = batch_size / avg_time\n",
    "        \n",
    "        speed_results.append({\n",
    "            'Model': model_name,\n",
    "            'Batch Size': batch_size,\n",
    "            'Avg Time (s)': avg_time,\n",
    "            'FPS': fps,\n",
    "            'Time per Sample (ms)': (avg_time / batch_size) * 1000\n",
    "        })\n",
    "        \n",
    "        print(f\"  Batch {batch_size}: {avg_time:.4f}s, {fps:.1f} FPS\")\n",
    "\n",
    "speed_df = pd.DataFrame(speed_results)\n",
    "print(\"\\nInference Speed Results:\")\n",
    "print(speed_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize inference speed results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# FPS by batch size\n",
    "for model_name in model_names:\n",
    "    model_data = speed_df[speed_df['Model'] == model_name]\n",
    "    axes[0, 0].plot(model_data['Batch Size'], model_data['FPS'], \n",
    "                   marker='o', label=model_name, linewidth=2)\n",
    "\n",
    "axes[0, 0].set_title('Frames Per Second by Batch Size', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Batch Size')\n",
    "axes[0, 0].set_ylabel('FPS')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Time per sample\n",
    "for model_name in model_names:\n",
    "    model_data = speed_df[speed_df['Model'] == model_name]\n",
    "    axes[0, 1].plot(model_data['Batch Size'], model_data['Time per Sample (ms)'], \n",
    "                   marker='s', label=model_name, linewidth=2)\n",
    "\n",
    "axes[0, 1].set_title('Time per Sample by Batch Size', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Batch Size')\n",
    "axes[0, 1].set_ylabel('Time per Sample (ms)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Speed vs Parameters scatter plot\n",
    "batch_1_data = speed_df[speed_df['Batch Size'] == 1]\n",
    "model_params = [model_summaries[name]['total_parameters'] for name in batch_1_data['Model']]\n",
    "model_fps = batch_1_data['FPS'].values\n",
    "\n",
    "axes[1, 0].scatter(model_params, model_fps, s=100, alpha=0.7, c=range(len(model_params)), cmap='viridis')\n",
    "for i, name in enumerate(batch_1_data['Model']):\n",
    "    axes[1, 0].annotate(name, (model_params[i], model_fps[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "axes[1, 0].set_title('Speed vs Model Complexity (Batch Size 1)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Number of Parameters')\n",
    "axes[1, 0].set_ylabel('FPS')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency heatmap (FPS per million parameters)\n",
    "efficiency_data = []\n",
    "for _, row in batch_1_data.iterrows():\n",
    "    model_name = row['Model']\n",
    "    params_m = model_summaries[model_name]['total_parameters'] / 1e6\n",
    "    efficiency = row['FPS'] / params_m\n",
    "    efficiency_data.append(efficiency)\n",
    "\n",
    "bars = axes[1, 1].bar(batch_1_data['Model'], efficiency_data, \n",
    "                     color='orange', alpha=0.8)\n",
    "axes[1, 1].set_title('Speed Efficiency\\n(FPS per Million Parameters)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('FPS per Million Parameters')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, eff in zip(bars, efficiency_data):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{eff:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze memory usage for different models\n",
    "if torch.cuda.is_available():\n",
    "    memory_results = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"\\nAnalyzing memory usage for {model_name}...\")\n",
    "        \n",
    "        for batch_size in [1, 4, 8, 16]:\n",
    "            try:\n",
    "                # Clear cache\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "                \n",
    "                dummy_input = torch.randn(batch_size, 1, 256, 256).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.no_grad():\n",
    "                    output = model(dummy_input)\n",
    "                \n",
    "                # Get memory stats\n",
    "                memory_allocated = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "                memory_reserved = torch.cuda.memory_reserved() / 1024**2   # MB\n",
    "                peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "                \n",
    "                memory_results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Batch Size': batch_size,\n",
    "                    'Allocated (MB)': memory_allocated,\n",
    "                    'Reserved (MB)': memory_reserved,\n",
    "                    'Peak (MB)': peak_memory,\n",
    "                    'Memory per Sample (MB)': memory_allocated / batch_size\n",
    "                })\n",
    "                \n",
    "                print(f\"  Batch {batch_size}: {memory_allocated:.1f} MB allocated, {peak_memory:.1f} MB peak\")\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(f\"  Batch {batch_size}: Out of memory\")\n",
    "                    memory_results.append({\n",
    "                        'Model': model_name,\n",
    "                        'Batch Size': batch_size,\n",
    "                        'Allocated (MB)': np.nan,\n",
    "                        'Reserved (MB)': np.nan,\n",
    "                        'Peak (MB)': np.nan,\n",
    "                        'Memory per Sample (MB)': np.nan\n",
    "                    })\n",
    "                else:\n",
    "                    raise e\n",
    "    \n",
    "    memory_df = pd.DataFrame(memory_results)\n",
    "    print(\"\\nMemory Usage Results:\")\n",
    "    print(memory_df.round(2))\n",
    "    \n",
    "else:\n",
    "    print(\"CUDA not available - skipping memory analysis\")\n",
    "    memory_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize memory usage if CUDA is available\n",
    "if not memory_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Memory usage by batch size\n",
    "    for model_name in model_names:\n",
    "        model_data = memory_df[memory_df['Model'] == model_name]\n",
    "        valid_data = model_data.dropna()\n",
    "        if not valid_data.empty:\n",
    "            axes[0].plot(valid_data['Batch Size'], valid_data['Allocated (MB)'], \n",
    "                        marker='o', label=model_name, linewidth=2)\n",
    "    \n",
    "    axes[0].set_title('Memory Usage by Batch Size', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Batch Size')\n",
    "    axes[0].set_ylabel('Memory Allocated (MB)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Memory per sample\n",
    "    batch_1_memory = memory_df[memory_df['Batch Size'] == 1].dropna()\n",
    "    if not batch_1_memory.empty:\n",
    "        bars = axes[1].bar(batch_1_memory['Model'], batch_1_memory['Memory per Sample (MB)'], \n",
    "                          color='purple', alpha=0.8)\n",
    "        axes[1].set_title('Memory per Sample (Batch Size 1)', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_ylabel('Memory per Sample (MB)')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, mem in zip(bars, batch_1_memory['Memory per Sample (MB)']):\n",
    "            height = bar.get_height()\n",
    "            axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{mem:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No memory usage data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Map Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image for feature analysis\n",
    "dataset = MedicalImageDataset(\n",
    "    data_dir='../archive',\n",
    "    metadata_file='../archive/overview.csv',\n",
    "    target_size=(256, 256),\n",
    "    split=None\n",
    ")\n",
    "\n",
    "# Get a sample image\n",
    "sample_image, _, _, sample_id = dataset[10]\n",
    "sample_image = sample_image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "print(f\"Analyzing feature maps for sample {sample_id}\")\n",
    "print(f\"Input image shape: {sample_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature maps from U-Net model\n",
    "unet_model = models.get('UNet_Medium')\n",
    "if unet_model is not None:\n",
    "    unet_model = unet_model.to(device)\n",
    "    unet_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get feature maps at different scales\n",
    "        feature_maps = unet_model.get_feature_maps(sample_image)\n",
    "    \n",
    "    print(f\"U-Net feature maps:\")\n",
    "    for i, fm in enumerate(feature_maps):\n",
    "        print(f\"  Level {i}: {fm.shape}\")\n",
    "    \n",
    "    # Visualize feature maps\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(sample_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "    axes[0, 0].set_title('Original Image', fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Feature maps at different levels\n",
    "    for i, fm in enumerate(feature_maps[:3]):\n",
    "        # Show first channel of feature map\n",
    "        feature_map = fm[0, 0].cpu().numpy()  # First batch, first channel\n",
    "        axes[0, i+1].imshow(feature_map, cmap='viridis')\n",
    "        axes[0, i+1].set_title(f'U-Net Level {i}\\n{fm.shape[2]}x{fm.shape[3]}, {fm.shape[1]} channels', \n",
    "                              fontweight='bold')\n",
    "        axes[0, i+1].axis('off')\n",
    "    \n",
    "    # Show multiple channels from one level\n",
    "    if len(feature_maps) > 1:\n",
    "        selected_fm = feature_maps[1]  # Second level\n",
    "        for j in range(min(4, selected_fm.shape[1])):\n",
    "            channel_map = selected_fm[0, j].cpu().numpy()\n",
    "            axes[1, j].imshow(channel_map, cmap='viridis')\n",
    "            axes[1, j].set_title(f'Channel {j}', fontweight='bold')\n",
    "            axes[1, j].axis('off')\n",
    "    \n",
    "    plt.suptitle('U-Net Feature Map Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"U-Net model not available for feature analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from DeepLabV3+ model\n",
    "deeplabv3_model = models.get('DeepLabV3Plus')\n",
    "if deeplabv3_model is not None:\n",
    "    deeplabv3_model = deeplabv3_model.to(device)\n",
    "    deeplabv3_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get features from decoder\n",
    "        decoder_features = deeplabv3_model.extract_features(sample_image)\n",
    "    \n",
    "    print(f\"DeepLabV3+ decoder features shape: {decoder_features.shape}\")\n",
    "    \n",
    "    # Visualize DeepLabV3+ features\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(sample_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "    axes[0, 0].set_title('Original Image', fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Show different channels from decoder features\n",
    "    num_channels_to_show = min(7, decoder_features.shape[1])\n",
    "    for i in range(num_channels_to_show):\n",
    "        row = i // 4\n",
    "        col = (i + 1) % 4 if row == 0 else i % 4\n",
    "        \n",
    "        if row < 2 and col < 4:\n",
    "            feature_map = decoder_features[0, i].cpu().numpy()\n",
    "            axes[row, col].imshow(feature_map, cmap='viridis')\n",
    "            axes[row, col].set_title(f'DeepLabV3+ Channel {i}', fontweight='bold')\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(num_channels_to_show + 1, 8):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        if row < 2:\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    plt.suptitle('DeepLabV3+ Feature Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"DeepLabV3+ model not available for feature analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Complexity vs Performance Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "comprehensive_comparison = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    if model_name in model_summaries:\n",
    "        summary = model_summaries[model_name]\n",
    "        \n",
    "        # Get speed data (batch size 1)\n",
    "        speed_data = speed_df[(speed_df['Model'] == model_name) & (speed_df['Batch Size'] == 1)]\n",
    "        fps = speed_data['FPS'].iloc[0] if not speed_data.empty else 0\n",
    "        time_per_sample = speed_data['Time per Sample (ms)'].iloc[0] if not speed_data.empty else 0\n",
    "        \n",
    "        # Get memory data if available\n",
    "        memory_per_sample = 0\n",
    "        if not memory_df.empty:\n",
    "            memory_data = memory_df[(memory_df['Model'] == model_name) & (memory_df['Batch Size'] == 1)]\n",
    "            if not memory_data.empty and not pd.isna(memory_data['Memory per Sample (MB)'].iloc[0]):\n",
    "                memory_per_sample = memory_data['Memory per Sample (MB)'].iloc[0]\n",
    "        \n",
    "        comprehensive_comparison.append({\n",
    "            'Model': model_name,\n",
    "            'Parameters (M)': summary['total_parameters'] / 1e6,\n",
    "            'Size (MB)': summary['model_size_mb'],\n",
    "            'FPS': fps,\n",
    "            'Time per Sample (ms)': time_per_sample,\n",
    "            'Memory per Sample (MB)': memory_per_sample,\n",
    "            'Efficiency Score': fps / (summary['total_parameters'] / 1e6)  # FPS per million parameters\n",
    "        })\n",
    "\n",
    "comp_df = pd.DataFrame(comprehensive_comparison)\n",
    "print(\"Comprehensive Model Comparison:\")\n",
    "print(comp_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create radar chart for model comparison\n",
    "import math\n",
    "\n",
    "# Normalize metrics for radar chart (higher is better)\n",
    "metrics = ['Speed', 'Efficiency', 'Memory Efficiency', 'Simplicity']\n",
    "normalized_data = {}\n",
    "\n",
    "for _, row in comp_df.iterrows():\n",
    "    model_name = row['Model']\n",
    "    \n",
    "    # Normalize metrics (0-1 scale, higher is better)\n",
    "    speed_norm = row['FPS'] / comp_df['FPS'].max()\n",
    "    efficiency_norm = row['Efficiency Score'] / comp_df['Efficiency Score'].max()\n",
    "    memory_eff_norm = (1 / (row['Memory per Sample (MB)'] + 1)) if row['Memory per Sample (MB)'] > 0 else 0.5\n",
    "    simplicity_norm = (1 / row['Parameters (M)']) / (1 / comp_df['Parameters (M)']).max()\n",
    "    \n",
    "    normalized_data[model_name] = [speed_norm, efficiency_norm, memory_eff_norm, simplicity_norm]\n",
    "\n",
    "# Create radar chart\n",
    "angles = [n / float(len(metrics)) * 2 * math.pi for n in range(len(metrics))]\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for i, (model_name, values) in enumerate(normalized_data.items()):\n",
    "    values += values[:1]  # Complete the circle\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i % len(colors)])\n",
    "    ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Model Performance Radar Chart\\n(Higher values are better)', \n",
    "             size=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommendations and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL COMPARISON AND ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n🏆 PERFORMANCE RANKINGS:\")\n",
    "\n",
    "# Speed ranking\n",
    "speed_ranking = comp_df.sort_values('FPS', ascending=False)\n",
    "print(\"\\n   Speed (FPS):\")\n",
    "for i, (_, row) in enumerate(speed_ranking.iterrows(), 1):\n",
    "    print(f\"   {i}. {row['Model']}: {row['FPS']:.1f} FPS\")\n",
    "\n",
    "# Efficiency ranking\n",
    "efficiency_ranking = comp_df.sort_values('Efficiency Score', ascending=False)\n",
    "print(\"\\n   Efficiency (FPS per Million Parameters):\")\n",
    "for i, (_, row) in enumerate(efficiency_ranking.iterrows(), 1):\n",
    "    print(f\"   {i}. {row['Model']}: {row['Efficiency Score']:.2f}\")\n",
    "\n",
    "# Parameter count ranking (fewer is better)\n",
    "param_ranking = comp_df.sort_values('Parameters (M)', ascending=True)\n",
    "print(\"\\n   Model Simplicity (Fewer Parameters):\")\n",
    "for i, (_, row) in enumerate(param_ranking.iterrows(), 1):\n",
    "    print(f\"   {i}. {row['Model']}: {row['Parameters (M)']:.1f}M parameters\")\n",
    "\n",
    "print(\"\\n📊 KEY INSIGHTS:\")\n",
    "\n",
    "# Find best performers\n",
    "fastest_model = speed_ranking.iloc[0]['Model']\n",
    "most_efficient = efficiency_ranking.iloc[0]['Model']\n",
    "simplest_model = param_ranking.iloc[0]['Model']\n",
    "\n",
    "print(f\"   • Fastest Model: {fastest_model}\")\n",
    "print(f\"   • Most Efficient: {most_efficient}\")\n",
    "print(f\"   • Simplest Model: {simplest_model}\")\n",
    "\n",
    "print(\"\\n🎯 RECOMMENDATIONS:\")\n",
    "\n",
    "print(\"\\n   For Real-time Applications:\")\n",
    "print(f\"   → Use {fastest_model} for maximum speed\")\n",
    "print(f\"   → Consider {most_efficient} for best speed/complexity trade-off\")\n",
    "\n",
    "print(\"\\n   For Resource-Constrained Environments:\")\n",
    "print(f\"   → Use {simplest_model} for minimal memory footprint\")\n",
    "print(f\"   → Consider {most_efficient} for balanced performance\")\n",
    "\n",
    "print(\"\\n   For High Accuracy Requirements:\")\n",
    "print(\"   → DeepLabV3+ typically provides better accuracy due to:\")\n",
    "print(\"     - Atrous convolutions for multi-scale context\")\n",
    "print(\"     - ASPP module for feature aggregation\")\n",
    "print(\"   → U-Net provides good accuracy with faster training\")\n",
    "\n",
    "print(\"\\n   For Multi-task Learning:\")\n",
    "print(\"   → U-Net architecture is more suitable due to:\")\n",
    "print(\"     - Skip connections preserve spatial information\")\n",
    "print(\"     - Easier to add multiple output heads\")\n",
    "print(\"     - Better gradient flow for joint training\")\n",
    "\n",
    "print(\"\\n🔧 OPTIMIZATION SUGGESTIONS:\")\n",
    "print(\"   • Use mixed precision training to reduce memory usage\")\n",
    "print(\"   • Implement gradient accumulation for larger effective batch sizes\")\n",
    "print(\"   • Consider model pruning for deployment optimization\")\n",
    "print(\"   • Use knowledge distillation to create smaller models\")\n",
    "print(\"   • Implement dynamic batching for variable input sizes\")\n",
    "\n",
    "print(\"\\n✅ CONCLUSION:\")\n",
    "print(\"   The choice of model depends on your specific requirements:\")\n",
    "print(\"   - Speed vs Accuracy trade-off\")\n",
    "print(\"   - Available computational resources\")\n",
    "print(\"   - Deployment constraints\")\n",
    "print(\"   - Task complexity (classification vs segmentation)\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}