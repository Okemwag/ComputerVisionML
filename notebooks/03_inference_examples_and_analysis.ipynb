{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Examples and Analysis\n",
    "\n",
    "This notebook demonstrates how to perform inference on real medical images\n",
    "and analyze the results using trained models.\n",
    "\n",
    "## Requirements Addressed:\n",
    "- 5.1: Visualize model predictions and performance metrics\n",
    "- 5.2: Generate loss curves and metric plots\n",
    "- 5.3: Create confusion matrices and ROC curves\n",
    "- 5.4: Generate overlay visualizations with color-coded regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from src.model import create_model\n",
    "from src.dataset import MedicalImageDataset\n",
    "from src.loaders import ImageLoader\n",
    "from src.preprocessing import MedicalImagePreprocessor\n",
    "from src.visualization import VisualizationEngine\n",
    "from src.metrics import MedicalMetrics\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pre-trained Model (Simulated)\n",
    "\n",
    "In a real scenario, you would load a trained model checkpoint. Here we'll create a model and simulate training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model configuration\n",
    "model_config = {\n",
    "    'model': {\n",
    "        'architecture': 'unet',\n",
    "        'task_type': 'classification',\n",
    "        'in_channels': 1,\n",
    "        'num_classes': 2,\n",
    "        'depth': 4,\n",
    "        'start_filters': 64,\n",
    "        'dropout': 0.2\n",
    "    },\n",
    "    'data': {\n",
    "        'image_size': [256, 256],\n",
    "        'normalize_method': 'hounsfield',\n",
    "        'window_type': 'soft_tissue'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = create_model(model_config)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model created and moved to {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Note: In practice, you would load trained weights here:\n",
    "# checkpoint = torch.load('path/to/trained/model.pth')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dataset = MedicalImageDataset(\n",
    "    data_dir='../archive',\n",
    "    metadata_file='../archive/overview.csv',\n",
    "    target_size=(256, 256),\n",
    "    normalize_method='hounsfield',\n",
    "    split='test'\n",
    ")\n",
    "\n",
    "print(f\"Test dataset loaded with {len(test_dataset)} samples\")\n",
    "\n",
    "# Create data loader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Data loader created with batch size 16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single Image Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample image for detailed analysis\n",
    "sample_idx = 5\n",
    "sample_image, sample_target, sample_age, sample_id = test_dataset[sample_idx]\n",
    "\n",
    "print(f\"Sample Analysis:\")\n",
    "print(f\"  Sample ID: {sample_id}\")\n",
    "print(f\"  True Label: {'Contrast' if sample_target == 1 else 'No Contrast'}\")\n",
    "print(f\"  Age: {sample_age:.0f}\")\n",
    "print(f\"  Image Shape: {sample_image.shape}\")\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    input_tensor = sample_image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    output = model(input_tensor)\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1)\n",
    "    confidence = torch.max(probabilities, dim=1)[0]\n",
    "\n",
    "# Convert to numpy for visualization\n",
    "probs_np = probabilities.cpu().numpy().squeeze()\n",
    "pred_class = predicted_class.cpu().item()\n",
    "conf_score = confidence.cpu().item()\n",
    "\n",
    "print(f\"\\nInference Results:\")\n",
    "print(f\"  Predicted: {'Contrast' if pred_class == 1 else 'No Contrast'}\")\n",
    "print(f\"  Confidence: {conf_score:.4f}\")\n",
    "print(f\"  Probabilities: No Contrast={probs_np[0]:.4f}, Contrast={probs_np[1]:.4f}\")\n",
    "print(f\"  Correct: {pred_class == sample_target.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize single image inference\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Original image\n",
    "image_np = sample_image.squeeze().numpy()\n",
    "axes[0, 0].imshow(image_np, cmap='gray')\n",
    "axes[0, 0].set_title(f'Medical Image\\nSample ID: {sample_id}', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Prediction probabilities\n",
    "class_names = ['No Contrast', 'Contrast']\n",
    "colors = ['lightcoral', 'lightblue']\n",
    "bars = axes[0, 1].bar(class_names, probs_np, color=colors, alpha=0.8)\n",
    "axes[0, 1].set_title(f'Prediction Probabilities\\nPredicted: {class_names[pred_class]}', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Probability')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, prob in zip(bars, probs_np):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{prob:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Confidence visualization\n",
    "conf_colors = ['red' if conf_score < 0.7 else 'orange' if conf_score < 0.9 else 'green']\n",
    "axes[0, 2].bar(['Confidence'], [conf_score], color=conf_colors, alpha=0.8)\n",
    "axes[0, 2].set_title(f'Prediction Confidence\\n{conf_score:.4f}', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Confidence Score')\n",
    "axes[0, 2].set_ylim(0, 1)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Image statistics\n",
    "axes[1, 0].hist(image_np.flatten(), bins=50, alpha=0.7, color='blue')\n",
    "axes[1, 0].set_title('Pixel Value Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Pixel Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Ground truth vs prediction\n",
    "true_label = class_names[sample_target.item()]\n",
    "pred_label = class_names[pred_class]\n",
    "is_correct = pred_class == sample_target.item()\n",
    "\n",
    "result_text = f\"Ground Truth: {true_label}\\nPrediction: {pred_label}\\nAge: {sample_age:.0f}\\n\\n\"\n",
    "result_text += f\"Result: {'âœ“ CORRECT' if is_correct else 'âœ— INCORRECT'}\\n\"\n",
    "result_text += f\"Confidence: {conf_score:.4f}\"\n",
    "\n",
    "color = 'lightgreen' if is_correct else 'lightcoral'\n",
    "axes[1, 1].text(0.5, 0.5, result_text, ha='center', va='center', \n",
    "               transform=axes[1, 1].transAxes, fontsize=12,\n",
    "               bbox=dict(boxstyle='round', facecolor=color, alpha=0.8))\n",
    "axes[1, 1].set_title('Prediction Summary', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Feature importance (simulated)\n",
    "# In practice, you might use techniques like Grad-CAM\n",
    "feature_importance = np.random.rand(5)\n",
    "feature_names = ['Intensity', 'Texture', 'Contrast', 'Edges', 'Spatial']\n",
    "axes[1, 2].barh(feature_names, feature_importance, color='purple', alpha=0.7)\n",
    "axes[1, 2].set_title('Feature Importance\\n(Simulated)', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Importance Score')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Inference and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform batch inference on test set\n",
    "all_predictions = []\n",
    "all_probabilities = []\n",
    "all_targets = []\n",
    "all_sample_ids = []\n",
    "all_ages = []\n",
    "\n",
    "model.eval()\n",
    "print(\"Performing batch inference...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, targets, ages, sample_ids) in enumerate(tqdm(test_loader, desc=\"Processing batches\")):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        \n",
    "        # Store results\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_sample_ids.extend(sample_ids)\n",
    "        all_ages.extend(ages.cpu().numpy())\n",
    "        \n",
    "        # Limit for demonstration (remove in practice)\n",
    "        if batch_idx >= 5:  # Process only first few batches for demo\n",
    "            break\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "all_targets = np.array(all_targets)\n",
    "all_ages = np.array(all_ages)\n",
    "\n",
    "print(f\"Processed {len(all_predictions)} samples\")\n",
    "print(f\"Overall Accuracy: {np.mean(all_predictions == all_targets):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze batch results\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Basic metrics\n",
    "accuracy = np.mean(all_predictions == all_targets)\n",
    "correct_mask = all_predictions == all_targets\n",
    "\n",
    "# Classification report\n",
    "class_names = ['No Contrast', 'Contrast']\n",
    "report = classification_report(all_targets, all_predictions, target_names=class_names, output_dict=True)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_predictions)\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(all_targets, all_probabilities[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Confidence analysis\n",
    "confidences = np.max(all_probabilities, axis=1)\n",
    "\n",
    "print(\"Batch Inference Analysis:\")\n",
    "print(f\"  Total Samples: {len(all_predictions)}\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  AUC: {roc_auc:.4f}\")\n",
    "print(f\"  Mean Confidence: {np.mean(confidences):.4f}\")\n",
    "print(f\"  Correct Predictions Confidence: {np.mean(confidences[correct_mask]):.4f}\")\n",
    "if np.any(~correct_mask):\n",
    "    print(f\"  Incorrect Predictions Confidence: {np.mean(confidences[~correct_mask]):.4f}\")\n",
    "\n",
    "print(f\"\\nPer-class metrics:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    key = class_name.lower().replace(' ', '_')\n",
    "    if key in report:\n",
    "        metrics = report[key]\n",
    "        print(f\"  {class_name}:\")\n",
    "        print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"    Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"    F1-Score: {metrics['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization of results\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "im = axes[0, 0].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[0, 0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "tick_marks = np.arange(len(class_names))\n",
    "axes[0, 0].set_xticks(tick_marks)\n",
    "axes[0, 0].set_xticklabels(class_names)\n",
    "axes[0, 0].set_yticks(tick_marks)\n",
    "axes[0, 0].set_yticklabels(class_names)\n",
    "axes[0, 0].set_ylabel('True Label')\n",
    "axes[0, 0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    axes[0, 0].text(j, i, format(cm[i, j], 'd'),\n",
    "                   horizontalalignment=\"center\",\n",
    "                   color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                   fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. ROC Curve\n",
    "axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[0, 1].set_xlim([0.0, 1.0])\n",
    "axes[0, 1].set_ylim([0.0, 1.05])\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(loc=\"lower right\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Confidence Distribution\n",
    "axes[0, 2].hist(confidences, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[0, 2].axvline(np.mean(confidences), color='red', linestyle='--', \n",
    "                  label=f'Mean: {np.mean(confidences):.3f}')\n",
    "axes[0, 2].set_title('Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Confidence')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Confidence by Correctness\n",
    "correct_conf = confidences[correct_mask]\n",
    "incorrect_conf = confidences[~correct_mask]\n",
    "\n",
    "axes[1, 0].hist(correct_conf, bins=15, alpha=0.7, label='Correct', color='green')\n",
    "if len(incorrect_conf) > 0:\n",
    "    axes[1, 0].hist(incorrect_conf, bins=15, alpha=0.7, label='Incorrect', color='red')\n",
    "axes[1, 0].set_title('Confidence by Correctness', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Confidence')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Age vs Accuracy\n",
    "age_bins = np.linspace(all_ages.min(), all_ages.max(), 8)\n",
    "age_centers = (age_bins[:-1] + age_bins[1:]) / 2\n",
    "age_accuracies = []\n",
    "\n",
    "for i in range(len(age_bins) - 1):\n",
    "    mask = (all_ages >= age_bins[i]) & (all_ages < age_bins[i + 1])\n",
    "    if np.any(mask):\n",
    "        age_accuracies.append(np.mean(correct_mask[mask]))\n",
    "    else:\n",
    "        age_accuracies.append(0)\n",
    "\n",
    "axes[1, 1].bar(age_centers, age_accuracies, width=np.diff(age_bins)[0] * 0.8, \n",
    "              alpha=0.7, color='orange')\n",
    "axes[1, 1].set_title('Accuracy by Age Group', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Age')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Class-wise Confidence\n",
    "class_0_conf = confidences[all_targets == 0]\n",
    "class_1_conf = confidences[all_targets == 1]\n",
    "\n",
    "axes[1, 2].hist(class_0_conf, bins=15, alpha=0.7, label=class_names[0], color='lightcoral')\n",
    "axes[1, 2].hist(class_1_conf, bins=15, alpha=0.7, label=class_names[1], color='lightblue')\n",
    "axes[1, 2].set_title('Confidence by True Class', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Confidence')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Prediction Distribution\n",
    "pred_counts = np.bincount(all_predictions)\n",
    "axes[2, 0].pie(pred_counts, labels=class_names, autopct='%1.1f%%', \n",
    "              colors=['lightcoral', 'lightblue'], startangle=90)\n",
    "axes[2, 0].set_title('Prediction Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 8. True vs Predicted Scatter\n",
    "jitter = np.random.normal(0, 0.05, len(all_targets))\n",
    "axes[2, 1].scatter(all_targets + jitter, all_predictions + jitter, \n",
    "                  c=confidences, cmap='viridis', alpha=0.6)\n",
    "axes[2, 1].set_xlabel('True Class')\n",
    "axes[2, 1].set_ylabel('Predicted Class')\n",
    "axes[2, 1].set_title('True vs Predicted (colored by confidence)', fontsize=14, fontweight='bold')\n",
    "axes[2, 1].set_xticks([0, 1])\n",
    "axes[2, 1].set_xticklabels(class_names)\n",
    "axes[2, 1].set_yticks([0, 1])\n",
    "axes[2, 1].set_yticklabels(class_names)\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Performance Summary\n",
    "summary_text = f\"Performance Summary\\n\\n\"\n",
    "summary_text += f\"Total Samples: {len(all_predictions)}\\n\"\n",
    "summary_text += f\"Accuracy: {accuracy:.4f}\\n\"\n",
    "summary_text += f\"AUC: {roc_auc:.4f}\\n\\n\"\n",
    "summary_text += f\"Precision (avg): {report['macro avg']['precision']:.4f}\\n\"\n",
    "summary_text += f\"Recall (avg): {report['macro avg']['recall']:.4f}\\n\"\n",
    "summary_text += f\"F1-Score (avg): {report['macro avg']['f1-score']:.4f}\\n\\n\"\n",
    "summary_text += f\"Mean Confidence: {np.mean(confidences):.4f}\\n\"\n",
    "summary_text += f\"Std Confidence: {np.std(confidences):.4f}\"\n",
    "\n",
    "axes[2, 2].text(0.1, 0.9, summary_text, transform=axes[2, 2].transAxes, \n",
    "               fontsize=11, verticalalignment='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "axes[2, 2].set_title('Performance Summary', fontsize=14, fontweight='bold')\n",
    "axes[2, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze incorrect predictions\n",
    "incorrect_indices = np.where(~correct_mask)[0]\n",
    "correct_indices = np.where(correct_mask)[0]\n",
    "\n",
    "print(f\"Error Analysis:\")\n",
    "print(f\"  Correct predictions: {len(correct_indices)}\")\n",
    "print(f\"  Incorrect predictions: {len(incorrect_indices)}\")\n",
    "\n",
    "if len(incorrect_indices) > 0:\n",
    "    print(f\"\\nIncorrect Prediction Analysis:\")\n",
    "    \n",
    "    # Analyze by true class\n",
    "    false_positives = incorrect_indices[all_targets[incorrect_indices] == 0]  # Predicted contrast, actually no contrast\n",
    "    false_negatives = incorrect_indices[all_targets[incorrect_indices] == 1]  # Predicted no contrast, actually contrast\n",
    "    \n",
    "    print(f\"  False Positives (predicted contrast, actually no contrast): {len(false_positives)}\")\n",
    "    print(f\"  False Negatives (predicted no contrast, actually contrast): {len(false_negatives)}\")\n",
    "    \n",
    "    # Confidence analysis for errors\n",
    "    error_confidences = confidences[incorrect_indices]\n",
    "    print(f\"  Mean confidence of incorrect predictions: {np.mean(error_confidences):.4f}\")\n",
    "    print(f\"  Min confidence of incorrect predictions: {np.min(error_confidences):.4f}\")\n",
    "    print(f\"  Max confidence of incorrect predictions: {np.max(error_confidences):.4f}\")\n",
    "    \n",
    "    # Age analysis for errors\n",
    "    error_ages = all_ages[incorrect_indices]\n",
    "    correct_ages = all_ages[correct_indices]\n",
    "    print(f\"  Mean age of incorrect predictions: {np.mean(error_ages):.1f}\")\n",
    "    print(f\"  Mean age of correct predictions: {np.mean(correct_ages):.1f}\")\n",
    "    \n",
    "    # Show some error cases\n",
    "    print(f\"\\nSample Error Cases:\")\n",
    "    for i, idx in enumerate(incorrect_indices[:5]):  # Show first 5 errors\n",
    "        sample_id = all_sample_ids[idx]\n",
    "        true_class = class_names[all_targets[idx]]\n",
    "        pred_class = class_names[all_predictions[idx]]\n",
    "        conf = confidences[idx]\n",
    "        age = all_ages[idx]\n",
    "        \n",
    "        print(f\"  {i+1}. Sample {sample_id}: True={true_class}, Pred={pred_class}, Conf={conf:.3f}, Age={age:.0f}\")\n",
    "else:\n",
    "    print(\"  No incorrect predictions found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error analysis\n",
    "if len(incorrect_indices) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Error distribution by confidence\n",
    "    axes[0, 0].hist(confidences[correct_indices], bins=20, alpha=0.7, \n",
    "                   label='Correct', color='green', density=True)\n",
    "    axes[0, 0].hist(confidences[incorrect_indices], bins=20, alpha=0.7, \n",
    "                   label='Incorrect', color='red', density=True)\n",
    "    axes[0, 0].set_title('Confidence Distribution: Correct vs Incorrect', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Confidence')\n",
    "    axes[0, 0].set_ylabel('Density')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error distribution by age\n",
    "    axes[0, 1].hist(all_ages[correct_indices], bins=15, alpha=0.7, \n",
    "                   label='Correct', color='green', density=True)\n",
    "    axes[0, 1].hist(all_ages[incorrect_indices], bins=15, alpha=0.7, \n",
    "                   label='Incorrect', color='red', density=True)\n",
    "    axes[0, 1].set_title('Age Distribution: Correct vs Incorrect', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Age')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confidence vs Age scatter for errors\n",
    "    axes[1, 0].scatter(all_ages[correct_indices], confidences[correct_indices], \n",
    "                      alpha=0.6, label='Correct', color='green', s=30)\n",
    "    axes[1, 0].scatter(all_ages[incorrect_indices], confidences[incorrect_indices], \n",
    "                      alpha=0.8, label='Incorrect', color='red', s=50, marker='x')\n",
    "    axes[1, 0].set_title('Confidence vs Age', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Age')\n",
    "    axes[1, 0].set_ylabel('Confidence')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error type analysis\n",
    "    false_positives = incorrect_indices[all_targets[incorrect_indices] == 0]\n",
    "    false_negatives = incorrect_indices[all_targets[incorrect_indices] == 1]\n",
    "    \n",
    "    error_types = ['False Positives', 'False Negatives']\n",
    "    error_counts = [len(false_positives), len(false_negatives)]\n",
    "    colors = ['orange', 'purple']\n",
    "    \n",
    "    bars = axes[1, 1].bar(error_types, error_counts, color=colors, alpha=0.8)\n",
    "    axes[1, 1].set_title('Error Types', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, error_counts):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                        f'{count}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No errors to visualize - perfect predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Interpretability (Simulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate feature importance analysis\n",
    "# In practice, you would use techniques like Grad-CAM, LIME, or SHAP\n",
    "\n",
    "print(\"Model Interpretability Analysis (Simulated)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate feature importance for different regions\n",
    "np.random.seed(42)\n",
    "feature_names = ['Central Region', 'Upper Region', 'Lower Region', 'Left Region', 'Right Region',\n",
    "                'High Intensity Areas', 'Low Intensity Areas', 'Edge Features', 'Texture Features']\n",
    "\n",
    "# Simulate different importance for contrast vs no-contrast\n",
    "contrast_importance = np.random.rand(len(feature_names)) * 0.8 + 0.2\n",
    "no_contrast_importance = np.random.rand(len(feature_names)) * 0.6 + 0.1\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Feature importance for contrast detection\n",
    "y_pos = np.arange(len(feature_names))\n",
    "axes[0].barh(y_pos, contrast_importance, color='lightblue', alpha=0.8)\n",
    "axes[0].set_yticks(y_pos)\n",
    "axes[0].set_yticklabels(feature_names)\n",
    "axes[0].set_xlabel('Importance Score')\n",
    "axes[0].set_title('Feature Importance for Contrast Detection', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparative feature importance\n",
    "x = np.arange(len(feature_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width/2, contrast_importance, width, label='Contrast', \n",
    "           color='lightblue', alpha=0.8)\n",
    "axes[1].bar(x + width/2, no_contrast_importance, width, label='No Contrast', \n",
    "           color='lightcoral', alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Features')\n",
    "axes[1].set_ylabel('Importance Score')\n",
    "axes[1].set_title('Comparative Feature Importance', fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop 5 Features for Contrast Detection:\")\n",
    "top_indices = np.argsort(contrast_importance)[::-1][:5]\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"  {i}. {feature_names[idx]}: {contrast_importance[idx]:.3f}\")\n",
    "\n",
    "print(\"\\nTop 5 Features for No-Contrast Detection:\")\n",
    "top_indices = np.argsort(no_contrast_importance)[::-1][:5]\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"  {i}. {feature_names[idx]}: {no_contrast_importance[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"=\" * 80)\n",
    "print(\"INFERENCE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸ“Š OVERALL PERFORMANCE:\")\n",
    "print(f\"   â€¢ Total samples processed: {len(all_predictions)}\")\n",
    "print(f\"   â€¢ Overall accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"   â€¢ AUC score: {roc_auc:.4f}\")\n",
    "print(f\"   â€¢ Mean prediction confidence: {np.mean(confidences):.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ CLASS-SPECIFIC PERFORMANCE:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    key = class_name.lower().replace(' ', '_')\n",
    "    if key in report:\n",
    "        metrics = report[key]\n",
    "        print(f\"   â€¢ {class_name}:\")\n",
    "        print(f\"     - Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"     - Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"     - F1-Score: {metrics['f1-score']:.4f}\")\n",
    "        print(f\"     - Support: {metrics['support']} samples\")\n",
    "\n",
    "print(f\"\\nðŸ” ERROR ANALYSIS:\")\n",
    "if len(incorrect_indices) > 0:\n",
    "    print(f\"   â€¢ Total errors: {len(incorrect_indices)} ({len(incorrect_indices)/len(all_predictions)*100:.1f}%)\")\n",
    "    print(f\"   â€¢ Mean confidence of errors: {np.mean(confidences[incorrect_indices]):.4f}\")\n",
    "    print(f\"   â€¢ Mean age of error cases: {np.mean(all_ages[incorrect_indices]):.1f} years\")\n",
    "    \n",
    "    false_positives = incorrect_indices[all_targets[incorrect_indices] == 0]\n",
    "    false_negatives = incorrect_indices[all_targets[incorrect_indices] == 1]\n",
    "    print(f\"   â€¢ False positives: {len(false_positives)}\")\n",
    "    print(f\"   â€¢ False negatives: {len(false_negatives)}\")\n",
    "else:\n",
    "    print(f\"   â€¢ No errors detected - perfect performance!\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ CONFIDENCE ANALYSIS:\")\n",
    "high_conf_threshold = 0.9\n",
    "low_conf_threshold = 0.6\n",
    "high_conf_mask = confidences >= high_conf_threshold\n",
    "low_conf_mask = confidences <= low_conf_threshold\n",
    "\n",
    "print(f\"   â€¢ High confidence predictions (â‰¥{high_conf_threshold}): {np.sum(high_conf_mask)} ({np.sum(high_conf_mask)/len(confidences)*100:.1f}%)\")\n",
    "if np.any(high_conf_mask):\n",
    "    print(f\"     - Accuracy of high confidence: {np.mean(correct_mask[high_conf_mask]):.4f}\")\n",
    "\n",
    "print(f\"   â€¢ Low confidence predictions (â‰¤{low_conf_threshold}): {np.sum(low_conf_mask)} ({np.sum(low_conf_mask)/len(confidences)*100:.1f}%)\")\n",
    "if np.any(low_conf_mask):\n",
    "    print(f\"     - Accuracy of low confidence: {np.mean(correct_mask[low_conf_mask]):.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ¥ CLINICAL INSIGHTS:\")\n",
    "print(f\"   â€¢ Age correlation with accuracy: {np.corrcoef(all_ages, correct_mask.astype(int))[0,1]:.4f}\")\n",
    "print(f\"   â€¢ Mean age of contrast cases: {np.mean(all_ages[all_targets == 1]):.1f} years\")\n",
    "print(f\"   â€¢ Mean age of no-contrast cases: {np.mean(all_ages[all_targets == 0]):.1f} years\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "print(f\"   â€¢ Model Performance:\")\n",
    "if accuracy >= 0.95:\n",
    "    print(f\"     - Excellent performance, ready for clinical validation\")\n",
    "elif accuracy >= 0.90:\n",
    "    print(f\"     - Good performance, consider additional validation\")\n",
    "elif accuracy >= 0.80:\n",
    "    print(f\"     - Moderate performance, needs improvement\")\n",
    "else:\n",
    "    print(f\"     - Poor performance, requires significant improvement\")\n",
    "\n",
    "print(f\"   â€¢ Confidence Thresholding:\")\n",
    "if np.any(low_conf_mask):\n",
    "    print(f\"     - Consider flagging predictions with confidence < {low_conf_threshold} for manual review\")\n",
    "    print(f\"     - {np.sum(low_conf_mask)} samples would be flagged ({np.sum(low_conf_mask)/len(confidences)*100:.1f}%)\")\n",
    "\n",
    "print(f\"   â€¢ Error Analysis:\")\n",
    "if len(incorrect_indices) > 0:\n",
    "    if len(false_positives) > len(false_negatives):\n",
    "        print(f\"     - More false positives than negatives - model tends to over-predict contrast\")\n",
    "    elif len(false_negatives) > len(false_positives):\n",
    "        print(f\"     - More false negatives than positives - model tends to under-predict contrast\")\n",
    "    else:\n",
    "        print(f\"     - Balanced error distribution\")\n",
    "\n",
    "print(f\"   â€¢ Clinical Deployment:\")\n",
    "print(f\"     - Implement confidence-based decision support\")\n",
    "print(f\"     - Provide uncertainty quantification for radiologists\")\n",
    "print(f\"     - Consider ensemble methods for improved reliability\")\n",
    "print(f\"     - Establish monitoring for model drift in production\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"\n  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}