#!/usr/bin/env python3\n\"\"\"\nExample Inference Script: Batch Processing\n\nThis script demonstrates how to perform batch inference on multiple medical images\nusing trained models for contrast detection.\n\nRequirements Addressed:\n- 5.1: Visualize model predictions and performance metrics\n- 5.2: Generate loss curves and metric plots\n- 5.3: Create confusion matrices and ROC curves\n- 5.4: Generate overlay visualizations with color-coded regions\n\"\"\"\n\nimport sys\nimport os\nsys.path.append('../')\n\nimport argparse\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport yaml\nimport logging\nfrom tqdm import tqdm\nimport json\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n\n# Import custom modules\nfrom src.model import create_model\nfrom src.dataset import MedicalImageDataset\nfrom src.visualization import VisualizationEngine\nfrom src.metrics import MedicalMetrics\nfrom src.utils import setup_logging\n\ndef load_trained_model(model_path: str, config_path: str = None, device: str = 'cuda'):\n    \"\"\"Load a trained model from checkpoint.\"\"\"\n    \n    # Load checkpoint\n    checkpoint = torch.load(model_path, map_location=device)\n    \n    # Load configuration\n    if config_path and Path(config_path).exists():\n        with open(config_path, 'r') as f:\n            config = yaml.safe_load(f)\n    else:\n        # Try to get config from checkpoint\n        config = checkpoint.get('config', {})\n        if not config:\n            # Default configuration\n            config = {\n                'model': {\n                    'architecture': 'unet',\n                    'task_type': 'classification',\n                    'in_channels': 1,\n                    'num_classes': 2,\n                    'depth': 4,\n                    'start_filters': 64\n                }\n            }\n    \n    # Create model\n    model = create_model(config)\n    \n    # Load weights\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    model.eval()\n    \n    return model, config\n\ndef batch_inference(model, dataset, device: str = 'cuda', batch_size: int = 16):\n    \"\"\"Perform batch inference on dataset.\"\"\"\n    \n    from torch.utils.data import DataLoader\n    \n    dataloader = DataLoader(\n        dataset, \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=4,\n        pin_memory=True\n    )\n    \n    all_predictions = []\n    all_probabilities = []\n    all_targets = []\n    all_sample_ids = []\n    all_ages = []\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for images, targets, ages, sample_ids in tqdm(dataloader, desc=\"Processing batches\"):\n            images = images.to(device)\n            targets = targets.to(device)\n            \n            # Forward pass\n            outputs = model(images)\n            \n            if isinstance(outputs, dict):\n                # Multi-task model\n                classification_output = outputs.get('classification', outputs.get('segmentation'))\n            else:\n                # Single task model\n                classification_output = outputs\n            \n            # Get probabilities and predictions\n            probabilities = torch.softmax(classification_output, dim=1)\n            predictions = torch.argmax(probabilities, dim=1)\n            \n            # Store results\n            all_predictions.extend(predictions.cpu().numpy())\n            all_probabilities.extend(probabilities.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n            all_sample_ids.extend(sample_ids)\n            all_ages.extend(ages.cpu().numpy())\n    \n    return {\n        'predictions': np.array(all_predictions),\n        'probabilities': np.array(all_probabilities),\n        'targets': np.array(all_targets),\n        'sample_ids': all_sample_ids,\n        'ages': np.array(all_ages)\n    }\n\ndef analyze_results(results: dict, class_names: list = None):\n    \"\"\"Analyze batch inference results.\"\"\"\n    \n    if class_names is None:\n        class_names = ['No Contrast', 'Contrast']\n    \n    predictions = results['predictions']\n    probabilities = results['probabilities']\n    targets = results['targets']\n    sample_ids = results['sample_ids']\n    ages = results['ages']\n    \n    # Basic metrics\n    accuracy = np.mean(predictions == targets)\n    \n    # Classification report\n    report = classification_report(targets, predictions, target_names=class_names, output_dict=True)\n    \n    # Confusion matrix\n    cm = confusion_matrix(targets, predictions)\n    \n    # ROC curve\n    fpr, tpr, _ = roc_curve(targets, probabilities[:, 1])\n    roc_auc = auc(fpr, tpr)\n    \n    # Confidence analysis\n    confidences = np.max(probabilities, axis=1)\n    correct_mask = predictions == targets\n    \n    analysis = {\n        'accuracy': accuracy,\n        'classification_report': report,\n        'confusion_matrix': cm,\n        'roc_curve': {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc},\n        'confidence_stats': {\n            'mean_confidence': np.mean(confidences),\n            'correct_confidence': np.mean(confidences[correct_mask]),\n            'incorrect_confidence': np.mean(confidences[~correct_mask]) if np.any(~correct_mask) else 0,\n            'confidence_distribution': confidences\n        },\n        'age_analysis': {\n            'mean_age': np.mean(ages),\n            'age_accuracy_correlation': np.corrcoef(ages, correct_mask.astype(int))[0, 1]\n        }\n    }\n    \n    return analysis\n\ndef create_comprehensive_visualizations(results: dict, analysis: dict, output_dir: Path, class_names: list = None):\n    \"\"\"Create comprehensive visualizations of batch results.\"\"\"\n    \n    if class_names is None:\n        class_names = ['No Contrast', 'Contrast']\n    \n    viz_engine = VisualizationEngine(str(output_dir))\n    \n    # 1. Confusion Matrix\n    plt.figure(figsize=(8, 6))\n    cm = analysis['confusion_matrix']\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n    plt.colorbar()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    \n    # Add text annotations\n    thresh = cm.max() / 2.\n    for i, j in np.ndindex(cm.shape):\n        plt.text(j, i, format(cm[i, j], 'd'),\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\",\n                fontsize=14, fontweight='bold')\n    \n    plt.ylabel('True Label', fontsize=12)\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.tight_layout()\n    plt.savefig(output_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # 2. ROC Curve\n    plt.figure(figsize=(8, 6))\n    roc_data = analysis['roc_curve']\n    plt.plot(roc_data['fpr'], roc_data['tpr'], color='darkorange', lw=2,\n             label=f'ROC curve (AUC = {roc_data[\"auc\"]:.3f})')\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=12)\n    plt.ylabel('True Positive Rate', fontsize=12)\n    plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16, fontweight='bold')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(output_dir / 'roc_curve.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # 3. Confidence Distribution\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Overall confidence distribution\n    confidences = analysis['confidence_stats']['confidence_distribution']\n    correct_mask = results['predictions'] == results['targets']\n    \n    axes[0, 0].hist(confidences, bins=30, alpha=0.7, color='blue', edgecolor='black')\n    axes[0, 0].axvline(np.mean(confidences), color='red', linestyle='--', \n                      label=f'Mean: {np.mean(confidences):.3f}')\n    axes[0, 0].set_title('Overall Confidence Distribution', fontweight='bold')\n    axes[0, 0].set_xlabel('Confidence')\n    axes[0, 0].set_ylabel('Frequency')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Confidence by correctness\n    correct_conf = confidences[correct_mask]\n    incorrect_conf = confidences[~correct_mask]\n    \n    axes[0, 1].hist(correct_conf, bins=20, alpha=0.7, label='Correct', color='green')\n    if len(incorrect_conf) > 0:\n        axes[0, 1].hist(incorrect_conf, bins=20, alpha=0.7, label='Incorrect', color='red')\n    axes[0, 1].set_title('Confidence by Prediction Correctness', fontweight='bold')\n    axes[0, 1].set_xlabel('Confidence')\n    axes[0, 1].set_ylabel('Frequency')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Age vs Accuracy\n    ages = results['ages']\n    age_bins = np.linspace(ages.min(), ages.max(), 10)\n    age_centers = (age_bins[:-1] + age_bins[1:]) / 2\n    age_accuracies = []\n    \n    for i in range(len(age_bins) - 1):\n        mask = (ages >= age_bins[i]) & (ages < age_bins[i + 1])\n        if np.any(mask):\n            age_accuracies.append(np.mean(correct_mask[mask]))\n        else:\n            age_accuracies.append(0)\n    \n    axes[1, 0].bar(age_centers, age_accuracies, width=np.diff(age_bins)[0] * 0.8, \n                  alpha=0.7, color='purple')\n    axes[1, 0].set_title('Accuracy by Age Group', fontweight='bold')\n    axes[1, 0].set_xlabel('Age')\n    axes[1, 0].set_ylabel('Accuracy')\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    # Class-wise confidence\n    class_0_conf = confidences[results['targets'] == 0]\n    class_1_conf = confidences[results['targets'] == 1]\n    \n    axes[1, 1].hist(class_0_conf, bins=20, alpha=0.7, label=class_names[0], color='lightcoral')\n    axes[1, 1].hist(class_1_conf, bins=20, alpha=0.7, label=class_names[1], color='lightblue')\n    axes[1, 1].set_title('Confidence Distribution by True Class', fontweight='bold')\n    axes[1, 1].set_xlabel('Confidence')\n    axes[1, 1].set_ylabel('Frequency')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(output_dir / 'confidence_analysis.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # 4. Sample Predictions Visualization\n    # Show some correct and incorrect predictions\n    correct_indices = np.where(correct_mask)[0]\n    incorrect_indices = np.where(~correct_mask)[0]\n    \n    # Select samples to visualize\n    n_samples = min(8, len(correct_indices) + len(incorrect_indices))\n    n_correct = min(4, len(correct_indices))\n    n_incorrect = min(4, len(incorrect_indices))\n    \n    if n_correct > 0 or n_incorrect > 0:\n        fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n        \n        sample_count = 0\n        \n        # Show correct predictions\n        for i in range(n_correct):\n            if sample_count >= 8:\n                break\n            idx = correct_indices[i]\n            row = sample_count // 4\n            col = sample_count % 4\n            \n            # This would require access to the actual images\n            # For now, we'll create a placeholder\n            axes[row, col].text(0.5, 0.5, \n                               f'Sample {results[\"sample_ids\"][idx]}\\n'\n                               f'True: {class_names[results[\"targets\"][idx]]}\\n'\n                               f'Pred: {class_names[results[\"predictions\"][idx]]}\\n'\n                               f'Conf: {np.max(results[\"probabilities\"][idx]):.3f}\\n'\n                               f'✓ CORRECT',\n                               ha='center', va='center', transform=axes[row, col].transAxes,\n                               fontsize=10, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n            axes[row, col].set_title(f'Correct Prediction {i+1}', fontweight='bold')\n            axes[row, col].axis('off')\n            sample_count += 1\n        \n        # Show incorrect predictions\n        for i in range(n_incorrect):\n            if sample_count >= 8:\n                break\n            idx = incorrect_indices[i]\n            row = sample_count // 4\n            col = sample_count % 4\n            \n            axes[row, col].text(0.5, 0.5, \n                               f'Sample {results[\"sample_ids\"][idx]}\\n'\n                               f'True: {class_names[results[\"targets\"][idx]]}\\n'\n                               f'Pred: {class_names[results[\"predictions\"][idx]]}\\n'\n                               f'Conf: {np.max(results[\"probabilities\"][idx]):.3f}\\n'\n                               f'✗ INCORRECT',\n                               ha='center', va='center', transform=axes[row, col].transAxes,\n                               fontsize=10, bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n            axes[row, col].set_title(f'Incorrect Prediction {i+1}', fontweight='bold')\n            axes[row, col].axis('off')\n            sample_count += 1\n        \n        # Hide unused subplots\n        for i in range(sample_count, 8):\n            row = i // 4\n            col = i % 4\n            axes[row, col].axis('off')\n        \n        plt.suptitle('Sample Predictions', fontsize=16, fontweight='bold')\n        plt.tight_layout()\n        plt.savefig(output_dir / 'sample_predictions.png', dpi=300, bbox_inches='tight')\n        plt.show()\n\ndef save_detailed_results(results: dict, analysis: dict, output_dir: Path, class_names: list = None):\n    \"\"\"Save detailed results to files.\"\"\"\n    \n    if class_names is None:\n        class_names = ['No Contrast', 'Contrast']\n    \n    # Create detailed results DataFrame\n    detailed_results = pd.DataFrame({\n        'sample_id': results['sample_ids'],\n        'true_class': results['targets'],\n        'true_class_name': [class_names[t] for t in results['targets']],\n        'predicted_class': results['predictions'],\n        'predicted_class_name': [class_names[p] for p in results['predictions']],\n        'confidence': np.max(results['probabilities'], axis=1),\n        'prob_no_contrast': results['probabilities'][:, 0],\n        'prob_contrast': results['probabilities'][:, 1],\n        'correct': results['predictions'] == results['targets'],\n        'age': results['ages']\n    })\n    \n    # Save to CSV\n    detailed_results.to_csv(output_dir / 'detailed_results.csv', index=False)\n    \n    # Save summary statistics\n    summary = {\n        'total_samples': len(results['predictions']),\n        'accuracy': float(analysis['accuracy']),\n        'auc': float(analysis['roc_curve']['auc']),\n        'mean_confidence': float(analysis['confidence_stats']['mean_confidence']),\n        'correct_confidence': float(analysis['confidence_stats']['correct_confidence']),\n        'incorrect_confidence': float(analysis['confidence_stats']['incorrect_confidence']),\n        'classification_report': analysis['classification_report'],\n        'confusion_matrix': analysis['confusion_matrix'].tolist()\n    }\n    \n    with open(output_dir / 'summary_statistics.json', 'w') as f:\n        json.dump(summary, f, indent=2)\n    \n    # Save classification report as text\n    with open(output_dir / 'classification_report.txt', 'w') as f:\n        f.write(\"Classification Report\\n\")\n        f.write(\"=\" * 50 + \"\\n\\n\")\n        \n        report = analysis['classification_report']\n        for class_name in class_names:\n            if class_name.lower().replace(' ', '_') in report:\n                class_metrics = report[class_name.lower().replace(' ', '_')]\n                f.write(f\"{class_name}:\\n\")\n                f.write(f\"  Precision: {class_metrics['precision']:.4f}\\n\")\n                f.write(f\"  Recall: {class_metrics['recall']:.4f}\\n\")\n                f.write(f\"  F1-Score: {class_metrics['f1-score']:.4f}\\n\")\n                f.write(f\"  Support: {class_metrics['support']}\\n\\n\")\n        \n        f.write(f\"Overall Accuracy: {analysis['accuracy']:.4f}\\n\")\n        f.write(f\"AUC: {analysis['roc_curve']['auc']:.4f}\\n\")\n\ndef main():\n    parser = argparse.ArgumentParser(description='Batch Inference Processing')\n    parser.add_argument('--model', type=str, required=True, \n                       help='Path to trained model checkpoint')\n    parser.add_argument('--data-dir', type=str, required=True,\n                       help='Path to data directory')\n    parser.add_argument('--metadata-file', type=str, required=True,\n                       help='Path to metadata CSV file')\n    parser.add_argument('--config', type=str, \n                       help='Path to model configuration file')\n    parser.add_argument('--output-dir', type=str, default='../outputs/batch_inference',\n                       help='Output directory for results')\n    parser.add_argument('--batch-size', type=int, default=16,\n                       help='Batch size for inference')\n    parser.add_argument('--device', type=str, default='cuda',\n                       help='Device to use for inference')\n    parser.add_argument('--split', type=str, default='test',\n                       choices=['train', 'val', 'test', None],\n                       help='Dataset split to use')\n    \n    args = parser.parse_args()\n    \n    # Create output directory\n    output_dir = Path(args.output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Setup logging\n    setup_logging(str(output_dir))\n    logger = logging.getLogger(__name__)\n    \n    # Set device\n    device = args.device if torch.cuda.is_available() else 'cpu'\n    logger.info(f\"Using device: {device}\")\n    \n    try:\n        # Load model\n        logger.info(f\"Loading model from: {args.model}\")\n        model, config = load_trained_model(args.model, args.config, device)\n        logger.info(\"Model loaded successfully\")\n        \n        # Create dataset\n        logger.info(f\"Loading dataset from: {args.data_dir}\")\n        dataset = MedicalImageDataset(\n            data_dir=args.data_dir,\n            metadata_file=args.metadata_file,\n            target_size=tuple(config.get('data', {}).get('image_size', [256, 256])),\n            normalize_method=config.get('data', {}).get('normalize_method', 'hounsfield'),\n            split=args.split\n        )\n        \n        logger.info(f\"Dataset loaded with {len(dataset)} samples\")\n        \n        # Perform batch inference\n        logger.info(\"Starting batch inference...\")\n        results = batch_inference(model, dataset, device, args.batch_size)\n        logger.info(\"Batch inference completed\")\n        \n        # Analyze results\n        logger.info(\"Analyzing results...\")\n        analysis = analyze_results(results)\n        \n        # Print summary\n        class_names = ['No Contrast', 'Contrast']\n        print(\"\\n\" + \"=\"*60)\n        print(\"BATCH INFERENCE RESULTS\")\n        print(\"=\"*60)\n        print(f\"Total Samples: {len(results['predictions'])}\")\n        print(f\"Overall Accuracy: {analysis['accuracy']:.4f}\")\n        print(f\"AUC: {analysis['roc_curve']['auc']:.4f}\")\n        print(f\"Mean Confidence: {analysis['confidence_stats']['mean_confidence']:.4f}\")\n        print(f\"Correct Predictions Confidence: {analysis['confidence_stats']['correct_confidence']:.4f}\")\n        if analysis['confidence_stats']['incorrect_confidence'] > 0:\n            print(f\"Incorrect Predictions Confidence: {analysis['confidence_stats']['incorrect_confidence']:.4f}\")\n        \n        print(f\"\\nPer-Class Results:\")\n        report = analysis['classification_report']\n        for i, class_name in enumerate(class_names):\n            key = class_name.lower().replace(' ', '_')\n            if key in report:\n                metrics = report[key]\n                print(f\"  {class_name}:\")\n                print(f\"    Precision: {metrics['precision']:.4f}\")\n                print(f\"    Recall: {metrics['recall']:.4f}\")\n                print(f\"    F1-Score: {metrics['f1-score']:.4f}\")\n                print(f\"    Support: {metrics['support']}\")\n        \n        print(\"=\"*60)\n        \n        # Create visualizations\n        logger.info(\"Creating visualizations...\")\n        create_comprehensive_visualizations(results, analysis, output_dir, class_names)\n        \n        # Save detailed results\n        logger.info(\"Saving detailed results...\")\n        save_detailed_results(results, analysis, output_dir, class_names)\n        \n        logger.info(f\"All results saved to: {output_dir}\")\n        logger.info(\"Batch inference completed successfully!\")\n        \n    except Exception as e:\n        logger.error(f\"Batch inference failed with error: {str(e)}\")\n        raise\n\nif __name__ == '__main__':\n    main()